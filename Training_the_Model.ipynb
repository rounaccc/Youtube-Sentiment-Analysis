{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg3reqs0XgAh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9ubIznXXkSl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from keras import regularizers\n",
        "import re\n",
        "from tqdm import tqdm_notebook\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import itertools\n",
        "from string import ascii_lowercase\n",
        "from functools import reduce\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToUMe0HNXs5d"
      },
      "outputs": [],
      "source": [
        "path='/content/drive/MyDrive/projects/Twitter Sentiment Analysis/final_dataframe.csv'\n",
        "df=pd.read_csv(path, encoding='latin')\n",
        "df.drop(['Unnamed: 0'],axis=1,inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFyOYONUXwzZ"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(how='any',axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRNuEbbHYBqh"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzvgE9yGRUHo"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYZCi7yefRHj"
      },
      "outputs": [],
      "source": [
        "y = df.sentiment.values  \n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ3f0MftYNJ-"
      },
      "outputs": [],
      "source": [
        "# processed_train_data=df.processed_tweets.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmYCF8TKYNHe"
      },
      "outputs": [],
      "source": [
        "# processed_train_data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i4kg65_m5KT"
      },
      "outputs": [],
      "source": [
        "RE_PATTERNS = {\n",
        "    ' sorry ' : [' soo rry '],\n",
        "    ' working ' : [' workingg '],\n",
        "    ' working ipod ' : [' workingipod '],\n",
        "    ' tonight ' : [' tonightt '],\n",
        "    ' fuck ' : [' fahk ', ' fcking '],\n",
        "    ' thank ' : [' thanx '],\n",
        "    ' come ' : [' comw '],\n",
        "    ' yuck ' : [' yuk '],\n",
        "    ' conversation ' : [' convo '],\n",
        "    ' i do not know ' : [' idunno '],\n",
        "    ' do not know ' : [' dunno ']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UWdNhl0BLeE"
      },
      "outputs": [],
      "source": [
        "# Text Normalization\n",
        "\n",
        "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
        "\n",
        "  if remove_patterns_text:\n",
        "    for target, patterns in RE_PATTERNS.items():\n",
        "      for pat in patterns:\n",
        "        text=str(text).replace(pat, target)\n",
        "  \n",
        "  return text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PQUSMWqRKHS"
      },
      "outputs": [],
      "source": [
        "final_tweets = [] \n",
        "for line in tqdm_notebook(df['processed_tweets'], total=1594993): \n",
        "    final_tweets.append(clean_text(line))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFFkJCVwYtmx"
      },
      "outputs": [],
      "source": [
        "final_tweets[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uijRhMDwZzt6"
      },
      "outputs": [],
      "source": [
        "max_features=222342\n",
        "maxpadlen = 170         \n",
        "val_split = 0.05\n",
        "embedding_dim_fasttext = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aFXTPRKYNFA"
      },
      "outputs": [],
      "source": [
        "#Tokenization\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(final_tweets))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(final_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf5sn75UYM8Z"
      },
      "outputs": [],
      "source": [
        "#Indexing\n",
        "word_index=tokenizer.word_index\n",
        "print(len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycrl0Qg_X4NJ"
      },
      "outputs": [],
      "source": [
        "#padding\n",
        "training_padded=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WGnRwrdX4Kj"
      },
      "outputs": [],
      "source": [
        "#Splitting data into Training and Validation Set\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(training_padded, y, test_size=0.05, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KQD_PmaX4HW"
      },
      "outputs": [],
      "source": [
        "embeddings_index_fasttext = {}\n",
        "f = open('/content/drive/MyDrive/projects/Kaggle competition - jigsaw/wiki-news-300d-1M.vec', encoding='utf8')\n",
        "for line in f:\n",
        "    line.encode('utf-8').strip()\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
        "f.close()\n",
        "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index_fasttext.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_fasttext[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drUmARqkejn0"
      },
      "source": [
        "**LSTM CNN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIvjlDkMoPrz"
      },
      "outputs": [],
      "source": [
        "model_1 = tf.keras.Sequential([\n",
        "\ttf.keras.layers.Embedding(len(word_index) + 1,\n",
        "                           embedding_dim_fasttext,\n",
        "                           weights = [embedding_matrix_fasttext],\n",
        "                           input_length = maxpadlen,\n",
        "                           trainable=False,\n",
        "                           name = 'embeddings'),\n",
        "  tf.keras.layers.Input(shape=(maxpadlen, ),dtype='int32'),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.1, return_sequences=True)),\n",
        "  tf.keras.layers.Conv1D(100, 5, activation='relu'),\n",
        "  tf.keras.layers.GlobalMaxPooling1D(),\n",
        "  tf.keras.layers.Dropout(.2),\n",
        "  tf.keras.layers.Dense(30, activation='relu', kernel_initializer='he_uniform'),\n",
        "\ttf.keras.layers.Dropout(.1),\n",
        "\ttf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')\n",
        "])\n",
        "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L_fqr68X3_y"
      },
      "outputs": [],
      "source": [
        "history = model_1.fit(x_train,y_train, epochs=10, batch_size=1024,  validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPqvySxacMRg"
      },
      "outputs": [],
      "source": [
        "acc,  val_acc  = history.history['accuracy'], history.history['val_accuracy']\n",
        "loss, val_loss = history.history['loss'], history.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br-nuYqOcMtj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def ConfusionMatrix(y_pred, y_test):\n",
        "    # Compute and plot the Confusion matrix\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    categories  = ['Negative','Positive']\n",
        "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
        "                xticklabels = categories, yticklabels = categories)\n",
        "\n",
        "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCTJ_3ticS4g"
      },
      "outputs": [],
      "source": [
        "# Predicting on the Test dataset.\n",
        "y_pred = model_1.predict(x_val)\n",
        "\n",
        "# Converting prediction to reflect the sentiment predicted.\n",
        "y_pred = np.where(y_pred>=0.5, 1, 0)\n",
        "\n",
        "# Printing out the Evaluation metrics. \n",
        "ConfusionMatrix(y_pred, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ9eH07ucpXb"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaqKKqB6h3Q1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "tf.keras.models.save_model(model_1, 'fast_text_model')\n",
        "pickle.dump(tokenizer, open('tokenizer.pickle', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9frf60xqrhdS"
      },
      "outputs": [],
      "source": [
        "tf.keras.models.save_model(model_1, 'fast_text_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_mkDBIlsY9H"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/file.zip /content/fast_text_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}